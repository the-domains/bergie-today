---
dateModified: '2016-05-31T10:22:45.969Z'
description: 'Artificial Intelligence performs gradient descent. The AI field discovers a path of success, and then travels that path until progress stops (when a local minimum is reached). Then, the field resets and chooses a new path, thus repeating the process. If this trend continues, AI should soon reach a local minimum, causing the next AI winter.'
inLanguage: en
isBasedOnUrl: 'http://kennethfriedman.org/projects/escaping-local-min/'
keywords:
  - neural
  - minsky
  - systems
  - url
  - intelligence
  - networks
  - artificial
  - minimum
  - nns
  - gradient
publisher:
  domain: kennethfriedman.org
  name: Kennethfriedman
  url: 'http://kennethfriedman.org'
title: Escaping the Local Minimum
author: []
starred: false
datePublished: '2016-05-31T10:23:06.650Z'
sourcePath: _posts/2016-05-31-escaping-the-local-minimum.md
inFeed: true
hasPage: false
inNav: false
_context: 'http://schema.org'
_type: MediaObject

---
<article style=""><img src="https://s3-us-west-2.amazonaws.com/the-grid-img/p/ead3cc7b1292148ef788f6bdb6c4ca7ae6ff5c72.png" /><h1>Escaping the Local Minimum</h1><p>Artificial Intelligence performs gradient descent. The AI field discovers a path of success, and then travels that path until progress stops (when a local minimum is reached). Then, the field resets and chooses a new path, thus repeating the process. If this trend continues, AI should soon reach a local minimum, causing the next AI winter.</p></article>